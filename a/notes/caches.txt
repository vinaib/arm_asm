1) cache stores both address of item in main memory and its data and some status
information.
2) with caches performance is improved
3) Reduces power consumption by avoiding the need to drive external signals
4) cahce Tag: 
The top bits of the 32-bit address tells the cache where the
information came from in main memory and is known as the tag.
The tag take up physical space in the cache.
5) size of cache:
The total cache size is a measure of the amount of data it can hold; the RAMs
used to hold tag values and status bits are not included in the calculation.

6) cache line:
It would be inefficient to hold one word of data for each tag address, so
several locations are typically grouped together under the same tag. This
logical block is commonly known as a cache line.

7) Identifying line:
The middle bits of the address, or index, identify the line.
The index is used as address for the cache RAMs and does not require storage as
a part of the tag. The index is the part of a memory address that determines in
which line(s) of the cache the address can be found.

A line refers to the smallest loadable unit of a cache, a block of contiguous
words from main memory.

The cache line contains tag, status bits and code or data.
8) offset: offset in line. Even this is not required to store in Tag RAM.
9) Associated with each line of data are one or more status bits.
instruction cache: Valid bit
Data Cache: valid bit, dirty bit
10) way:
A way is a subdivision of a cache, each way being of equal size and indexed in
the same fashion. The line associated with a particular index value from each
cache way grouped together forms a set.
11) cache controller:
-> is a hardware, that copies code or data from main memory to cache memroy
automatically.
-> interprets read and write memory requests before passing them on to the
memory controller. It process a request by dividing the address of the request
into three fields: tag field, set index field(line), data index field(offset).
-> controller uses "set index" portion of the address to locate the cache line
-> controller then checks valid bit to determine cache line is active
-> and compares cache tag to the tad field of the address.
-> Cache Hit/Miss:
if comparison succeeds it is hit else it is miss.

12) Cache line fill:
on a cache miss, the controller copies an entire cahce line from main memory to
cahce memory and provides the requested code or data to the processor. The
copying of a cache line from memory to cache memory is known as cache line fill.

13) Data Streaming:
During cache line fill, cache controller may forward the loading data to the
coer at the same time it is copying it to cache. Streaming allows a processor to
continue execution while the cache controller fills the remaining words in the
cache line.

14) Eviction:
If valid data exits in cache line but represents another address block in main
memory, the entire cahce line is evicted and replaced by the cahce line
containing the requested address. The process of removing an existing cache line
as part of servicing cache miss is known as eviction.


Desgin facts
------------
1) larger caches for more expensive chips
2) Making an internal core cache larger can potentially limit the max speed of
the core

How cahces improves performance?
--------------------------------
1) The improvement a cache provides is possible because computer programs
execute in nonrandom ways.
2) if program's access to memory were random, a cache would provide little
improvement to overall system performance.
3) The principle of locality of reference explains the performance improvement
providede by the addition of a cache memory to a system.


principle of locality of reference
-----------------------------------
This principle states that computer softeare programs frequently run small loops
of code that repeatedlt operate on local section of data memory. The repeated
use of the same code or data in memory is the reason a cache improves
performance.

Temporal and Spatial locality
------------------------------
The cache makes use of this repeated local reference in both time and space. 

Temporal locality:
if the reference is in time, it is called temproal locality. This means that
programs tend to reuse the same address over time.
Example: 
Code, for instance, can contain loops, meaning that the same code gets
executed repeatedly or a function can be called multiple times.

if the reference is by address, it is called spatial locality. This means tend
to use addresses that are near to each other.
Example:
Data accesses (for example, to the stack) can be limited to small regions of
memory.

Write Buffers
-------------
The write buffer is a block that decouples writes being done by the core when
executing store instructions from the external memory bus. The core places the
address, control and data values associated with the store into a set of
hardware buffers. Like the cache, it sits between the core and main memory. This
enables the core to move on and execute the next instructions without having to
stop and wait for the slow main memory to actually complete the write operation.

Cache Drawbacks
---------------
1) Non deterministic execution time:
some problems that are not present in an uncached core. One such drawback is
that program execution time can become non-deterministic. 

This means that the execution time of a particular piece of code can vary
significantly. This can be something of a problem in hard real-time systems
where strongly deterministic behavior is required.

2) External devices:
You require a way to control how different parts of memory are accessed by the
cache and write buffer.
Example:
In some cases, you want the core to read up-to-date data from an external
device, such as a peripheral. It would not be sensible to use a cached value of
a timer peripheral.

3) Coherency:
The contents of cache and external memory might not be the same. this is because
the processor can update the cache contents, which have not yet been written
back to main memory.

This can be a particular problem when you have multiple cores or memory agents
like an external DMA controller.

ARM L1 Cache: Harvard Architecture: has separate caches for Instruction and
data.

ARM L2 Cache: Unified Architecture: holds both instructions and data.
ARM L2C-310 is an example of such an external L2 cache controller block.

Systems having multiple cores, will have separate L1 cache and common L2 cache.





